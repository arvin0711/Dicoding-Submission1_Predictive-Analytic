# -*- coding: utf-8 -*-
"""Dicoding - ML Terapan_Submission 1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pSgG97ivdXsiA-frzSTJ5UcXU7NITOtP

# Data Understanding
"""

pip install keras-tuner

"""## Import and Load Dataset

Dataset yang digunakan dalam proyek ini merupakan dataset harga saham harian Tesla yang mencakup periode tertentu dari periode 06/29/2010 hingga 03/17/2017 yang terdiri dari 7 kolom dengan 1692 baris data. Dataset ini diperoleh dari Kaggle melalui tautan berikut: https://www.kaggle.com/datasets/rpaguirre/tesla-stock-price/data

* Date - Tanggal pencatatan harga saham.
* Open - Harga saham pada awal perdagangan.
* High - Harga tertinggi yang dicapai dalam satu hari perdagangan.
* Low - Harga terendah dalam satu hari perdagangan.
* Close - Harga penutupan saham pada akhir perdagangan.
* Adj Close - Harga penutupan yang disesuaikan.
* Volume - Jumlah saham yang diperdagangkan dalam satu hari.
"""

from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
import numpy as np
import pandas as pd
from keras.models import Sequential
from keras.layers import LSTM, Dense
from keras.optimizers import Adam
from sklearn.metrics import mean_absolute_percentage_error
from sklearn.model_selection import train_test_split
from keras.layers import LSTM, Dense, Dropout
import tensorflow as tf
import random


# Set seed untuk memastikan hasil yang konsisten
seed = 42
np.random.seed(seed)
tf.random.set_seed(seed)
random.seed(seed)

# Load data
df = pd.read_csv('/content/Tesla.csv - Tesla.csv.csv', index_col='Date', parse_dates=True)
#df = df.drop['Adj close']
df

df.plot(subplots=True,figsize=(10,20))

"""Kondisi Data:

* Semua variabel berbentuk numerik dengan tipe data float64 (Open, High, Low, Close, Adj Close) dan int64 (Volume).
* Missing Value: Tidak ditemukan nilai yang hilang karena semua kolom memiliki 1692 non-null count.
* Duplikasi: Tidak ada indikasi duplikasi berdasarkan informasi yang diberikan.



"""

df.info()

"""Berdasarkan statistik deskriptif, terdapat kemungkinan outlier pada kolom Volume, di mana nilai maksimum (37,163,900) jauh lebih besar dibandingkan kuartil ketiga (5,662,100). Oleh karena itu, pada tahap selanjutnya akan dilakukan pengecekan outliers



"""

df.describe()

"""Korelasi:

* Variabel Fitur Open, High, Low, memiliki korelasi yang sangat tinggi (mendekati 1), terhadap variabel target yaitu Close sehingga menunjukkan hubungan linear kuat..

* Fitur Volume memiliki korelasi rendah dengan harga saham (~0.40), yang menunjukkan bahwa perubahan harga tidak selalu berkaitan dengan jumlah volume perdagangan.

"""

df.corr()

"""## Deteksi Outlier

Berdasarkan visualisasi boxplot, terdapat outlier pada variabel Volume. Oleh karena itu, akan dilakukan pengecekan outlier tersebut. Kemudian, berikutnya outlier tersebut akan dihapus / didrop karena jumlahnya tidak begitu banyak sehingga tidak berpengaruh signifikan terhadap dataset yang ada
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Mengatur ukuran figure
plt.figure(figsize=(len(df.columns) * 4, 6))

# Loop untuk membuat boxplot vertikal untuk setiap kolom
for i, col in enumerate(df.columns, 1):
    plt.subplot(1, len(df.columns), i)  # Membuat subplot secara horizontal
    sns.boxplot(y=df[col])  # Boxplot vertikal
    plt.title(f"Boxplot {col}")

plt.tight_layout()  # Mengatur tata letak agar tidak bertumpuk
plt.show()

"""## Hapus Outlier

Penghapusan outliers dilakukan menggunakan metode IQR. Outlier berjumlah 80. Setelah dilakukan penghapusan baris data yang mengandung outliers, sehingga dihasilkan data bersih terdiri dari 1612 baris data dari sebelumnya berjumlah 1692 baris.
"""

import numpy as np
import pandas as pd

# Menghitung Q1 (Kuartil 25%) dan Q3 (Kuartil 75%)
Q1 = df['Volume'].quantile(0.25)
Q3 = df['Volume'].quantile(0.75)

# Menghitung IQR
IQR = Q3 - Q1

# Menentukan batas bawah dan batas atas untuk outlier
batas_bawah = Q1 - 1.5 * IQR
batas_atas = Q3 + 1.5 * IQR

#  Menghitung jumlah outlier
jumlah_outlier = ((df['Volume'] < batas_bawah) | (df['Volume'] > batas_atas)).sum()
print(f"Jumlah Outlier pada 'Volume': {jumlah_outlier}")

# **2. Menampilkan outlier**
outliers = df[(df['Volume'] < batas_bawah) | (df['Volume'] > batas_atas)]
print("\nOutlier pada 'Volume':")
print(outliers)

# **3. Menghapus outlier**
df_cleaned = df[(df['Volume'] >= batas_bawah) & (df['Volume'] <= batas_atas)]

print(f"\nJumlah data setelah menghapus outlier: {len(df_cleaned)} (Sebelumnya: {len(df)})")

"""# Data Preparation

# Pembagian Data Latih dan Uji

Pada tahap ini dilakukan pembagian data menjadi menjadi dua bagian yaitu data latih dan data uji. Pembagian data latih dan data uji ini menggunakan skenario 80:20. Variabel fitur terdiri dari kolom Open, High, Low, dan Volume, sedangkan variabel targetnya yaitu Close.
"""

#from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split

# Partisi data dengan train_test_split (tanpa shuffle, cocok untuk time series)
test_prop = 0.2
train_df, test_df = train_test_split(df_cleaned, test_size=test_prop, shuffle=False)


# Penentuan variabel X dan Y
train_features = train_df[['Open', 'High', 'Low', 'Volume']]
train_target = train_df[['Close']]

test_features = test_df[['Open', 'High', 'Low', 'Volume']]
test_target = test_df[['Close']]  # Target 'Close'

train_features

train_target

print("train fitur:", train_features.shape)
print("train target:", train_target.shape)
print("test fitur:", test_features.shape)
print("test target:", test_target.shape)

"""## Normalisasi

Pada tahap ini dilakukan normalisasi data dengan mengubah nilai-nilai dari suatu dataset ke dalam rentang nilai tertentu. Pada proyek ini normalisasi dilakukan menggunakan Metode Min Max sehingga diubah menjadi rentang 0 sampai 1. Tujuan utama normalisasi adalah untuk menghasilkan data yang konsisten sehingga setiap variabel memiliki pangaruh yang seimbang terhadap model yang dibangun
"""

# Normalisasi setiap fitur satu per satu di data pelatihan
scalers = {}
train_features_scaled = []

for feature in train_features.columns:
    scaler = MinMaxScaler(feature_range=(0, 1))
    scaled_feature = scaler.fit_transform(train_features[[feature]])
    scalers[feature] = scaler  # Simpan scaler untuk fitur ini
    train_features_scaled.append(scaled_feature)

# Gabungkan kembali fitur yang dinormalisasi
train_features_scaled = np.hstack(train_features_scaled)

# Normalisasi target secara terpisah
scaler_target = MinMaxScaler(feature_range=(0, 1))
train_target_scaled = scaler_target.fit_transform(train_target)

"""## Pembuatan Urutan Data Baru

Setelah melakukan normalisasi data, tahap berikutnya adalah pembuatan urutan data baru menjadi ukuran 3 dimensi (samples, timesteps, jumlah fitur) agar sesuai dengan input yang diperlukan oleh model biLSTM. Timesteps digunakan untuk menentukan jumlah data masa lalu yang diperhitungkan dalam memprediksi satu nilai di masa depan. Pada tahap ini timesteps yang digunakan adalah 9 sehingga setiap prediksi di masa depan mempertimbangkan 9data di masa lalu.
"""

# variabel penyimpan nilai timestep
timesteps = 9

# Penyesuaian struktur data pelatihan (multivariate forecasting)
train_data = np.hstack((train_features_scaled, train_target_scaled))
x_train = []
y_train = []
for i in range(timesteps, len(train_data)):
    x_train.append(train_data[i-timesteps:i, :-1])  # Mengambil semua fitur kecuali target
    y_train.append(train_data[i, -1])  # Target: 'AI'

# Mengonversi list ke numpy array setelah loop
x_train, y_train = np.array(x_train), np.array(y_train)

# Reshape data untuk input LSTM [samples, timesteps, features]
x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], train_features_scaled.shape[1]))  # Jumlah fitur input
y_train = y_train.reshape((y_train.shape[0], 1))  # Target tetap 1D

# Normalisasi setiap fitur satu per satu di data pengujian menggunakan scaler yang telah di-fit dari data pelatihan
test_features_scaled = []

for feature in test_features.columns:
    scaled_feature = scalers[feature].transform(test_features[[feature]])
    test_features_scaled.append(scaled_feature)

# Gabungkan kembali fitur yang dinormalisasi untuk data pengujian
test_features_scaled = np.hstack(test_features_scaled)

# Normalisasi target pengujian secara terpisah
test_target_scaled = scaler_target.transform(test_target)

# Penyesuaian struktur data pengujian
test_data = np.hstack((test_features_scaled, test_target_scaled))
x_test = []
y_test = []
for i in range(timesteps, len(test_data)):
    x_test.append(test_data[i-timesteps:i, :-1])  # Mengambil semua fitur kecuali target
    y_test.append(test_data[i, -1])  # Target: 'AI'

# Mengonversi list ke numpy array setelah loop
x_test, y_test = np.array(x_test), np.array(y_test)

# Reshape data untuk input LSTM [samples, timesteps, features]
x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], test_features_scaled.shape[1]))  # Jumlah fitur input
y_test = y_test.reshape((y_test.shape[0], 1))  # Target tetap 1D

train_features_scaled

np.set_printoptions(threshold=10)  # threshold menentukan berapa banyak elemen yang ditampilkan

(train_target_scaled)

"""Pada Gambar di bawah ini ditunjukkan bahwa ukuran data latih dan data uji sudah berubah menjadi ukuran 3 dimensi."""

print("Urutan data baru train fitur:", x_train.shape)
print("Urutan data baru train target:", y_train.shape)
print("Urutan data baru test fitur:", x_test.shape)
print("Urutan data baru test target:", y_test.shape)

"""# Modeling Bidirectional LSTM

Parameter terbaik yang diperoleh dari hasil tuning untuk model BiLSTM adalah sebagai berikut:
*   lstm_units: 50
*   dropout: 0.1
*   optimizer: adam

Setelah mendapatkan parameter terbaik dari hasil tuning, model final kemudian dibangun dan dilatih menggunakan parameter tersebut.
"""

import keras_tuner as kt
from keras.layers import LSTM, Dense, Dropout, Bidirectional
from keras.models import Sequential
from keras.regularizers import l2
import tensorflow as tf

# Fungsi untuk membuat model LSTM dengan parameter yang bisa dituning
def build_model(hp):
    model = Sequential()

    # Layer LSTM pertama
    model.add(Bidirectional(LSTM(hp.Choice('lstm_units', [50, 100]),
                                 activation='tanh', return_sequences=True,
                                 kernel_regularizer=l2(1e-5)),
                            input_shape=(x_train.shape[1], x_train.shape[2])))

    # Dropout layer
    model.add(Dropout(hp.Choice('dropout', [0.1, 0.2, 0.3])))

    # Layer LSTM kedua
    model.add(Bidirectional(LSTM(hp.Choice('lstm_units', [50, 100]),
                                 activation='tanh', kernel_regularizer=l2(1e-5))))

    # Dense layer
    model.add(Dense(1, kernel_regularizer=l2(1e-5)))

    # Compile model dengan loss function MSE
    model.compile(optimizer=hp.Choice('optimizer', ['adam', 'rmsprop']),
                  loss='mean_squared_error')

    return model

# Inisialisasi tuner Hyperband
tuner = kt.Hyperband(build_model,
                     objective='val_loss',
                     max_epochs=50,
                     factor=3,
                     directory='hyperband_tuning',
                     project_name='LSTM_Tuning_Limited')

# Early stopping untuk menghentikan training jika tidak ada peningkatan
early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Menjalankan tuning
tuner.search(x_train, y_train, epochs=50, validation_split=0.1, batch_size=64, callbacks=[early_stopping])

# Mendapatkan parameter terbaik
best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]

# Menampilkan parameter terbaik
print(f"""
Parameter terbaik yang ditemukan:
- lstm_units: {best_hps.get('lstm_units')}
- dropout: {best_hps.get('dropout')}
- optimizer: {best_hps.get('optimizer')}
""")

# Membuat model dengan parameter terbaik
best_model = tuner.hypermodel.build(best_hps)

# Melatih model dengan parameter terbaik
history = best_model.fit(x_train, y_train, epochs=50, validation_split=0.1, batch_size=64, callbacks=[early_stopping])

# Menampilkan loss terakhir
LastLoss = '%.4f' % history.history['val_loss'][-1]
print(f"Loss Terakhir dengan Parameter Terbaik: {LastLoss}")

plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.show()

"""## Prediksi Data Tes"""

# Prediksi menggunakan data pengujian
y_test_pred = best_model.predict(x_test)

# Denormalisasi hasil prediksi
y_test_pred = scaler_target.inverse_transform(y_test_pred)

y_test_pred[y_test_pred < 0] = 0
print(y_test_pred)

import matplotlib.pyplot as plt

# Denormalisasi target asli untuk data pengujian
y_test_denorm = scaler_target.inverse_transform(y_test)

# Plot hasil prediksi vs data aktual
plt.figure(figsize=(10, 6))

# Plot data aktual
y_test_denorm_series = pd.Series(y_test_denorm.ravel(), index=df.index[-len(y_test_denorm):])
y_test_pred_series = pd.Series(y_test_pred.ravel(), index=df.index[-len(y_test_pred):])


# Plotting
plt.figure(figsize=(10, 6))
plt.plot(y_test_denorm_series, label='Actual Data', color='blue')
plt.plot(y_test_pred_series, label='Predicted Data', color='red')


# Judul, label, dan pengaturan sumbu X
plt.title('Actual vs Predicted')
plt.xlabel('Date')
plt.ylabel('Saham Tesla')
plt.legend()
plt.gca().xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%Y'))
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""# Evaluation"""

from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import numpy as np

# Pastikan y_test_denorm dan y_test_pred sudah terdefinisi
# Menghitung Mean Squared Error (MSE)
mseLSTM = mean_squared_error(y_test_denorm, y_test_pred)

# Menghitung R-squared (R²)
r_squaredLSTM = r2_score(y_test_denorm, y_test_pred)

# Menghitung Mean Absolute Percentage Error (MAPE)
epsilon = 1e-10  # Untuk menghindari pembagian dengan nol
mapeLSTM = 100 * np.mean(np.abs((y_test_denorm - y_test_pred) / (y_test_denorm + epsilon)))

# Menghitung Mean Absolute Error (MAE)
maeLSTM = mean_absolute_error(y_test_denorm, y_test_pred)

# Menampilkan hasil evaluasi
print('Test MSE: %.4f' % mseLSTM)
print('R-squared: %.4f' % r_squaredLSTM)
print('MAPE: %.2f' % mapeLSTM)
print('MAE: %.2f' % maeLSTM)

"""# Forecast Masa Depan (30 hari)"""

# *Prediksi Masa Depan 30 Hari dengan Timestep 9*


# Gunakan data terakhir dari dataset pengujian sebagai input awal (timestep = 9)
future_input = test_data[-timesteps:, :-1]  # Ambil 9 timestep terakhir sebagai input awal
future_predictions = []

# Loop untuk memprediksi 180 hari ke depan
for _ in range(30):
    # Prediksi hari berikutnya berdasarkan 9 timestep terakhir
    pred = best_model.predict(future_input.reshape(1, timesteps, -1))

    # Simpan hasil prediksi
    future_predictions.append(pred[0, 0])  # Ambil nilai pertama hasil prediksi

    # Perbarui future_input dengan menggantikan data lama
    new_row = np.hstack((future_input[-1, 1:], pred[0]))
    future_input = np.vstack((future_input[1:], new_row))

# Denormalisasi hasil prediksi masa depan
future_predictions = scaler_target.inverse_transform(np.array(future_predictions).reshape(-1, 1))

# Buat index tanggal baru untuk prediksi masa depan
last_date = df.index[-1]  # Ambil tanggal terakhir dari dataset
future_dates = pd.date_range(start=last_date, periods=30, freq='D')  # Buat rentang 180 hari ke depan

# Konversi prediksi masa depan ke pandas Series
future_pred_series = pd.Series(future_predictions.ravel(), index=future_dates)

# -------------------------
# **Plot Data Aktual, Prediksi, dan Prediksi Masa Depan**
# -------------------------
plt.figure(figsize=(12, 6))

# Plot data aktual
plt.plot(y_test_denorm_series, label='Actual Data', color='blue')

# Plot hasil prediksi pada data uji
plt.plot(y_test_pred_series, label='Predicted Data', color='red')

# Plot prediksi masa depan
plt.plot(future_pred_series, label='Future Prediction (Next 30 Days)', color='green', linestyle='dashed')

# Judul dan label
plt.title('Stock Price Prediction: Actual vs Predicted vs Future Prediction')
plt.xlabel('Date')
plt.ylabel('Stock Price')
plt.legend()
plt.xticks(rotation=45)

# Format tanggal agar lebih rapi
plt.gca().xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%Y-%m-%d'))
plt.tight_layout()
plt.show()

from tabulate import tabulate

# Format ulang tanggal agar hanya menampilkan "YYYY-MM-DD"
formatted_dates = [date.strftime('%Y-%m-%d') for date in future_dates]

# Buat daftar tanggal prediksi dan nilai prediksi
data = list(zip(formatted_dates, future_predictions.ravel()))

# Buat header tabel
headers = ['Date', 'Prediction']

# Buat tabel dengan format yang lebih rapi
table = tabulate(data, headers=headers, tablefmt='fancy_grid')

# Cetak tabel
print(table)

"""# Model Bidirectional GRU

Parameter terbaik yang diperoleh dari hasil tuning untuk model BiGRU adalah sebagai berikut:
*   gru_units: 50
*   dropout: 0.3
*   optimizer: rmsprop

Setelah mendapatkan parameter terbaik dari hasil tuning, model final kemudian dibangun dan dilatih menggunakan parameter tersebut.
"""

import keras_tuner as kt
from keras.layers import GRU, Dense, Dropout, Bidirectional
from keras.models import Sequential
from keras.regularizers import l2
import tensorflow as tf

# Fungsi untuk membuat model GRU dengan parameter yang bisa dituning
def build_model(hp):
    model = Sequential()

    # Layer GRU pertama
    model.add(Bidirectional(GRU(hp.Int('gru_units', min_value=50, max_value=100, step=50),
                                activation='tanh', return_sequences=True,
                                kernel_regularizer=l2(1e-5)),
                             input_shape=(x_train.shape[1], x_train.shape[2])))

    # Dropout layer
    model.add(Dropout(hp.Choice('dropout', [0.1, 0.2, 0.3])))

    # Layer GRU kedua
    model.add(Bidirectional(GRU(hp.Int('gru_units', min_value=50, max_value=100, step=50),
                                activation='tanh', kernel_regularizer=l2(1e-5))))

    # Dense layer
    model.add(Dense(1, kernel_regularizer=l2(1e-5)))

    # Compile model dengan loss function MSE
    model.compile(optimizer=hp.Choice('optimizer', ['adam', 'rmsprop']),
                  loss='mean_squared_error')

    return model

# Inisialisasi tuner Hyperband
tuner = kt.Hyperband(build_model,
                     objective='val_loss',
                     max_epochs=50,
                     factor=3,
                     directory='hyperband_tuning',
                     project_name='GRU_Tuning')

# Early stopping untuk menghentikan training jika tidak ada peningkatan
early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Menjalankan tuning
tuner.search(x_train, y_train, epochs=50, validation_split=0.1, batch_size=64, callbacks=[early_stopping])

# Mendapatkan parameter terbaik
best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]

# Menampilkan parameter terbaik
print(f"""
Parameter terbaik yang ditemukan:
- gru_units: {best_hps.get('gru_units')}
- dropout: {best_hps.get('dropout')}
- optimizer: {best_hps.get('optimizer')}
""")

# Membuat model dengan parameter terbaik
best_model = tuner.hypermodel.build(best_hps)

# Melatih model dengan parameter terbaik
history = best_model.fit(x_train, y_train, epochs=50, validation_split=0.1, batch_size=64, callbacks=[early_stopping])

# Menampilkan loss terakhir
LastLoss = '%.4f' % history.history['val_loss'][-1]
print(f"Loss Terakhir dengan Parameter Terbaik: {LastLoss}")

"""## Prediksi Pada Data Tes Model Bidirectional GRU"""

# Prediksi menggunakan data pengujian
y_test_pred = best_model.predict(x_test)

# Denormalisasi hasil prediksi
y_test_pred = scaler_target.inverse_transform(y_test_pred)

y_test_pred[y_test_pred < 0] = 0
print(y_test_pred)

"""## Evaluasi Model BIGRU"""

from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import numpy as np

# Pastikan y_test_denorm dan y_test_pred sudah terdefinisi
# Menghitung Mean Squared Error (MSE)
mseGRU = mean_squared_error(y_test_denorm, y_test_pred)

# Menghitung R-squared (R²)
r_squaredGRU = r2_score(y_test_denorm, y_test_pred)

# Menghitung Mean Absolute Percentage Error (MAPE)
epsilon = 1e-10  # Untuk menghindari pembagian dengan nol
mapeGRU = 100 * np.mean(np.abs((y_test_denorm - y_test_pred) / (y_test_denorm + epsilon)))

# Menghitung Mean Absolute Error (MAE)
maeGRU = mean_absolute_error(y_test_denorm, y_test_pred)

# Menampilkan hasil evaluasi
print('Test MSE: %.4f' % mseGRU)
print('R-squared: %.4f' % r_squaredGRU)
print('MAPE: %.2f' % mapeGRU)
print('MAE: %.2f' % maeGRU)

# -------------------------
# **Prediksi Masa Depan 30 Hari dengan Timestep 9**
# -------------------------

# Gunakan data terakhir dari dataset pengujian sebagai input awal (timestep = 9)
future_input = test_data[-timesteps:, :-1]  # Ambil 9 timestep terakhir sebagai input awal
future_predictions = []

# Loop untuk memprediksi 180 hari ke depan
for _ in range(30):
    # Prediksi hari berikutnya berdasarkan 9 timestep terakhir
    pred = best_model.predict(future_input.reshape(1, timesteps, -1))

    # Simpan hasil prediksi
    future_predictions.append(pred[0, 0])  # Ambil nilai pertama hasil prediksi

    # Perbarui future_input dengan menggantikan data lama
    new_row = np.hstack((future_input[-1, 1:], pred[0]))
    future_input = np.vstack((future_input[1:], new_row))

# Denormalisasi hasil prediksi masa depan
future_predictions = scaler_target.inverse_transform(np.array(future_predictions).reshape(-1, 1))

# Buat index tanggal baru untuk prediksi masa depan
last_date = df.index[-1]  # Ambil tanggal terakhir dari dataset
future_dates = pd.date_range(start=last_date, periods=30, freq='D')  # Buat rentang 180 hari ke depan

# Konversi prediksi masa depan ke pandas Series
future_pred_series = pd.Series(future_predictions.ravel(), index=future_dates)

# -------------------------
# **Plot Data Aktual, Prediksi, dan Prediksi Masa Depan**
# -------------------------
plt.figure(figsize=(12, 6))

# Plot data aktual
plt.plot(y_test_denorm_series, label='Actual Data', color='blue')

# Plot hasil prediksi pada data uji
plt.plot(y_test_pred_series, label='Predicted Data', color='red')

# Plot prediksi masa depan
plt.plot(future_pred_series, label='Future Prediction (Next 30 Days)', color='green', linestyle='dashed')

# Judul dan label
plt.title('Stock Price Prediction: Actual vs Predicted vs Future')
plt.xlabel('Date')
plt.ylabel('Stock Price')
plt.legend()
plt.xticks(rotation=45)

# Format tanggal agar lebih rapi
plt.gca().xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%Y-%m-%d'))
plt.tight_layout()
plt.show()

from tabulate import tabulate

# Format ulang tanggal agar hanya menampilkan "YYYY-MM-DD"
formatted_dates = [date.strftime('%Y-%m-%d') for date in future_dates]

# Buat daftar tanggal prediksi dan nilai prediksi
data = list(zip(formatted_dates, future_predictions.ravel()))

# Buat header tabel
headers = ['Date', 'Prediction']

# Buat tabel dengan format yang lebih rapi
table = tabulate(data, headers=headers, tablefmt='fancy_grid')

# Cetak tabel
print(table)

"""# Perbandingan BILSTM dan BIGRU"""

import pandas as pd

# Membuat DataFrame untuk perbandingan
comparison_df = pd.DataFrame({
    "Metrik": ["MSE", "R-squared", "MAPE (%)", "MAE"],
    "BILSTM": [mseLSTM, r_squaredLSTM, mapeLSTM, maeLSTM],
    "BIGRU": [mseGRU, r_squaredGRU, mapeGRU, maeGRU]
})

# Menampilkan tabel
print(tabulate(comparison_df, headers='keys', tablefmt='grid'))

"""## Analisis Hasil Evaluasi:
* MSE (Mean Squared Error): BiGRU memiliki nilai MSE yang lebih rendah dibandingkan BiLSTM, menunjukkan bahwa model ini menghasilkan error yang lebih kecil.
*  MAPE (Mean Absolute Percentage Error): BiGRU memiliki MAPE yang lebih rendah, menunjukkan bahwa kesalahan relatif model ini lebih kecil dibandingkan BiLSTM.
*   MAE (Mean Absolute Error): BiGRU memiliki MAE yang lebih kecil, yang berarti model ini menghasilkan prediksi yang lebih akurat secara absolut.
*  R-Squared (R²): BiGRU memiliki nilai R² lebih tinggi, menunjukkan bahwa model ini lebih mampu menjelaskan variasi dalam data dibandingkan BiLSTM.

## Dampak terhadap Business Understanding
Model prediksi harga saham yang dikembangkan memiliki dampak signifikan terhadap pemahaman bisnis, terutama dalam pengambilan keputusan investasi. Berikut adalah beberapa aspek utama dampaknya:



1.  Menjawab Problem Statement dan Mencapai Goals
*  Model yang dikembangkan mampu memprediksi harga saham berdasarkan data historis dengan tingkat akurasi yang baik, membantu investor dalam membuat keputusan lebih tepat.
*  Penggunaan BiLSTM dan BiGRU menunjukkan bahwa deep learning efektif dalam memprediksi harga saham, metrik evaluasi menunjukkan bahwa BiGRU lebih optimal dengan error yang lebih rendah dan nilai R² yang lebih tinggi
* Variabel fitur Open, High, dan Low memiliki pengaruh besar terhadap variabel target Close (mendekati 1) dalam prediksi harga saham. Sementara itu, variabel fitur Volume menunjukkan korelasi yang rendah dengan variabel Close (0,40) yang menunjukkan bahwa perubahan harga tidak selalu berkaitan dengan jumlah volume perdagangan.



2.  Dampak dari Solusi Statement
*   Penggunaan BiLSTM dan BiGRU membuktikan bahwa deep learning dapat menangkap pola harga saham dengan baik, memberikan metode yang lebih canggih dibandingkan pendekatan konvensional.
*   Hasil tuning hyperparameter yang dilakukan berhasil meningkatkan kinerja model secara signifikan, menjadikan model lebih akurat dan andal
*  Model yang dikembangkan dapat menjadi acuan bagi investor dan analis dalam pengambilan keputusan bisnis terkait investasi saham, sehingga dapat meminimalkan risiko dan memaksimalkan keuntungan.









.

## Kesimpulan
Berdasarkan hasil evaluasi, model algoritma BiGRU dipilih sebagai model terbaik untuk memprediksi harga saham. Hal ini didasarkan pada metrik evaluasi yang menunjukkan bahwa BiGRU memiliki error yang lebih rendah (MSE, MAE, dan MAPE) serta nilai R² yang lebih tinggi dibandingkan BiLSTM. Selain itu, BiGRU lebih efisien dalam proses pelatihan dibandingkan BiLSTM, sehingga lebih optimal untuk digunakan dalam peramalan harga saham.
"""